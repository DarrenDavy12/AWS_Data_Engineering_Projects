### Data Lake with Glue & Athena

### Transform a collection of raw CSV/JSON files into a queryable serverless data lake.

<br>

####  Step 1: Uploaded csv to the raw folder in the S3 bucket to convert into a JSON file shown in the processed folder. 
This allows me to attach data to 'Glue database' to generate table schema based off the data and finally be ready to be uploaded to 'athena' for querying later on in the later steps. 


#### The data is called 'users_activity' 


<br>


![Image](https://github.com/user-attachments/assets/2770c749-6e49-4c90-93e3-f98703ecfd01)


<br>


![Image](https://github.com/user-attachments/assets/0dcf0c40-6548-4362-9e2c-f116867ff1e3)



<br>


![Image](https://github.com/user-attachments/assets/872a4c50-7f9a-4d25-857b-9df6f1d38c66)


<br>


#### After the upload was successfull, I went into the processed folder to download the new .JSON file and viewed the file in vscode. 

<br>

![Image](https://github.com/user-attachments/assets/1ce13598-5fa6-40d8-851d-54e45ff75523)


<br>

![Image](https://github.com/user-attachments/assets/33028f24-1661-4665-8e8a-dafbac9c74ea)



<br>


### Step 2: Create a role named "AWSGlueServiceRole` with the following permissions:
     - `AmazonS3ReadOnlyAccess`
     - `AWSGlueServiceRole`
     - `AmazonAthenaFullAccess`

<br>


![image](https://github.com/user-attachments/assets/f8967f71-94e1-46a3-b6af-d416a99eb0b5)


<br>


![image](https://github.com/user-attachments/assets/6afa5b6e-8345-47b9-8e1e-f65c0c6ce131)


<br>


![image](https://github.com/user-attachments/assets/7f45b288-fb23-4223-96cf-55cba81d2d5e)

<br>


### Step 4: Stream Processing with Kinesis & Lambda

Simulate real-time user events, stream them into Amazon Kinesis, transform them using Lambda, and store the output in S3.


In the kinesis homepage, I created a data stream with a capacity of one provisioned shard, and then clicked create data stream. 

<br> 

![Image](https://github.com/user-attachments/assets/66b055b9-0692-413a-a9dd-7652803816d1)


<br>


![Image](https://github.com/user-attachments/assets/ce8a9c85-6ccc-4458-acb0-b77193436528)

<br>

I used a python script to create the events that will be happening in the stream process. I named it 'kinesis_producer.py'


![Image](https://github.com/user-attachments/assets/f2e2b536-d239-465f-9e8a-2637cc51ec8f)


<br>

### Step 3: Created a Lambda to Consume Stream 

I went to the Lambda Console, navigated to the AWS Console onwards to Lambda, once done I clicked on “Create function”

I chose Author from scratch with the function name being 'kinesis_consumer_lambda' and the Runtime being Python 3.11

Permission I chose was "Create a new role with basic Lambda permissions" then clicked on "Create function".

![Image](https://github.com/user-attachments/assets/0cf5f7c4-09eb-45a9-89a2-5f77090aeb23)


<br>


#### Step 4: Added Kinesis as a Trigger
In the Lambda function page, I scrolled to "Function overview", clicked on “Add trigger" chose Kinesis,
stream user-activity-stream (the stream you created), batch size was 100 (default),
the starting position was "TRIM_HORIZON", clicked "Add". 

<br>

![Image](https://github.com/user-attachments/assets/799666a3-9806-4b15-81d3-1cb5d3bf6484)

<br>

### Step 3: Run an AWS Glue Crawler to catalog the data.

### Step 4: Use Athena to run SQL queries against the catalog.

### Step 5: Optimize with Parquet + compression.
